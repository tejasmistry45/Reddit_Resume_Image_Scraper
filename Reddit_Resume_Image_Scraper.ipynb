{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Resume image Scraper By visiting All available Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-gpu\") \n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "search_url = \"https://www.reddit.com/r/resumes/search/?q=computer+science+resume+india\"\n",
    "driver.get(search_url)\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.TAG_NAME, \"reddit-feed\"))\n",
    ")\n",
    "\n",
    "# Create a folder to save images if it doesn't exist\n",
    "if not os.path.exists('downloaded_images'):\n",
    "    os.makedirs('downloaded_images')\n",
    "\n",
    "# Initialize counter for downloaded images and a set to track visited URLs\n",
    "image_count = 1  \n",
    "\n",
    "supported_extensions = ['.jpg', '.jpeg', '.png', '.webp']\n",
    "\n",
    "# Keep track of processed URLs to avoid duplicates\n",
    "processed_urls = set()\n",
    "\n",
    "# Main scraping loop: iterating over each link in search results\n",
    "while True:\n",
    "    \n",
    "    reddit_feed = driver.find_element(By.TAG_NAME, \"reddit-feed\")\n",
    "    aria_links = reddit_feed.find_elements(By.CSS_SELECTOR, \"a[aria-label]\")\n",
    "    \n",
    "    # Process each aria link found\n",
    "    for link in aria_links:\n",
    "        post_url = link.get_attribute(\"href\")\n",
    "        \n",
    "        # Skip the link if it's already been processed\n",
    "        if not post_url or post_url in processed_urls:\n",
    "            continue\n",
    "\n",
    "        # Add the URL to the set of processed URLs\n",
    "        processed_urls.add(post_url)\n",
    "\n",
    "        # Open the post in a new tab\n",
    "        driver.execute_script(\"window.open(arguments[0], '_blank');\", post_url)\n",
    "        driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "        # Wait for the post page to load and image to be visible\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'media-lightbox-img'))\n",
    "            )\n",
    "\n",
    "            # Find the first image div within media-lightbox-img\n",
    "            media_divs = driver.find_elements(By.CLASS_NAME, 'media-lightbox-img')\n",
    "            if media_divs:\n",
    "                img = media_divs[0].find_element(By.TAG_NAME, 'img')\n",
    "                img_url = img.get_attribute('src')\n",
    "\n",
    "                if img_url:\n",
    "                    print(f\"Found image URL: {img_url}\")\n",
    "                    \n",
    "                    # Clean URL for file extension\n",
    "                    parsed_url = urlparse(img_url)\n",
    "                    base_url = parsed_url.path  \n",
    "                    \n",
    "                    # Check if URL ends with a supported extension and download\n",
    "                    if any(base_url.endswith(ext) for ext in supported_extensions):\n",
    "                        image_name = f\"image{image_count}.{base_url.split('.')[-1]}\"\n",
    "                        image_path = os.path.join('downloaded_images', image_name)\n",
    "\n",
    "                        # Download the image\n",
    "                        try:\n",
    "                            response = requests.get(img_url, timeout=10)\n",
    "                            response.raise_for_status()\n",
    "                            with open(image_path, 'wb') as file:\n",
    "                                file.write(response.content)\n",
    "                            print(f\"Downloaded image: {image_path}\")\n",
    "                            image_count += 1\n",
    "                        except requests.exceptions.RequestException as e:\n",
    "                            print(f\"Failed to download image from {img_url}. Error: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Skipping unsupported image URL: {img_url}\")\n",
    "            else:\n",
    "                print(\"No media divs with images found in this post.\")\n",
    "\n",
    "            # Close the tab after processing\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing link: {post_url}. Error: {e}\")\n",
    "            driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "        \n",
    "        # Pause between iterations\n",
    "        time.sleep(1)\n",
    "\n",
    "    # If no new links are found, end the loop\n",
    "    if not aria_links:\n",
    "        break\n",
    "\n",
    "print(f\"Total images downloaded: {image_count - 1}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text From Images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education:\n",
      "Education\n",
      "ABC University\n",
      "\n",
      "Programming Languages: Java, JavaScript, Python, SQL,\n",
      "Web Teclnologies & Databases: ITML, CSS, Sas/SCSS, MySQL, MongoDB, PostgreSQL.\n",
      "Framoworks & Libraries: React, Reus, React Router, Nodes, Express, mongoose, Semantic UI, Bootstrap\n",
      "Developer Tools: Git, GitKrakeu, GitHlub, Postman, npm, Webpack, Babel, WekStorm, VS Code, Eeipss, PyCharm\n",
      "\n",
      "Projects\n",
      "\n",
      "Project 1\n",
      "\n",
      "+ Engineered a robotic ear that\n",
      "‘object recognition using tensor”\n",
      "\n",
      "+ Programmed autonomous navigation ofthe robot ad inplemented spooch synthenis\n",
      "\n",
      "Mer 2021 ~ Jun 2021\n",
      "provide navigation ail forthe bd and visually inpaired by perforiing Rea\n",
      "\n",
      "«Spak and Python\n",
      "+ Achieved object recognition of prevalent items with 905 accuracy’ by implementing coco dataset sing tensortlow ite\n",
      "+ Technologies Used: Raspberry Pi, Python, TensorFlow, OpeuCV, NuaaPy\n",
      "\n",
      "Project 2 | Wetsite Li May 2021 ~ Jun 2021\n",
      "+ Developed an e-commerce platform along with an onder processing workflow and Content Management System (CMS)\n",
      "+ Integrated PayPal APL to authorize payments and implemented dynanie filters for various products\n",
      "\n",
      "+ Designed a responsive layout that will render dif\n",
      "\n",
      "+ Technologies Used: React, Redus, React Router,\n",
      "\n",
      "ut React components based on sereen width\n",
      "Nodes, Express, MongoDB, mongoose, Bootstrap\n",
      "\n",
      "Project 3 | Website Link | Source Code Apr 2021 ~ May 2021\n",
      "+ Developed a fullstack web application that allows users to erate & manage a entalogue of tasks\n",
      "\n",
      "+ Inplemeuted Google OAuth 2.0 for user authentication and designee! REST API endpoints with MongoDE integration\n",
      "+ Technologies Used: React, Reds, React Router, Node.j, Express, MongeDB, mongoose, Passport js, Bootstrap\n",
      "Project 4 | Wetsi\n",
      "+ Developed a fullstack we ap\n",
      "\n",
      "Mar 2021 ~ Apr 2021\n",
      "bbe nuthoized to get feedback fom various users by’ sending Sk-+ emails at once\n",
      "+ Integrated Stripe APL to authorize payments and implemented Google OAuth 20 for user authentiention\n",
      "\n",
      "1 Sendgrid API to deliver bulk email athe fondback recieved will be tabulated using Reaet components\n",
      "+ Technologies Used: React, Redus, Renet Router, Node,j, Express, MongoDB, mongoose, Passport js, Semantic UL\n",
      "\n",
      "+ Tntogeat\n",
      "\n",
      "Project & | Website Link | Source Code Feb 2021 ~ Mar 2021\n",
      "+ Developed! a web application that can perform lve-sreaming using Real-Time Messaging Protocol (RTMP) ser\n",
      "+ Produced functional components for an interactive ser interface ving React Hooks and Semantic UL\n",
      "\n",
      "+ Technologies Used: Reset, Rodus, Reet Router, Semantic UL\n",
      "\n",
      "Activities\n",
      "\n",
      "Core Member (Club Name) | ABC University ‘Mar 2020 ~ May 2021\n",
      "\n",
      "+ Create coding challenges, expert seminars, and various technical events in collaboration with 2 core mernbers to bel\n",
      "students learn new technologies and compete\n",
      "\n",
      "Head Coordinator (Post Name) | ABC\n",
      "\n",
      "+ Organized CMRU's\n",
      "\n",
      "city Mar 2021 ~ Apr 2021\n",
      "st online tech fest and developed the official website si €85 and Vania JavnSeript\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files/Tesseract-OCR/tesseract.exe\"  \n",
    "\n",
    "image_path = \"D:/Machine Learning/WEB_Scraping/new2/downloaded_images/image4.jpg\"  \n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Extract text using pytesseract\n",
    "extracted_text = pytesseract.image_to_string(img)\n",
    "\n",
    "# Define sections we want to extract\n",
    "sections = {\n",
    "    \"Contact Info\": r\"NAME SURNAME[\\s\\S]+?(?=SKILLS)\",\n",
    "    \"Skills\": r\"SKILLS[\\s\\S]+?(?=PROJECTS)\",\n",
    "    \"Projects\": r\"PROJECTS[\\s\\S]+?(?=WORK EXPERIENCE)\",\n",
    "    \"Work Experience\": r\"WORK EXPERIENCE[\\s\\S]+?(?=EDUCATION)\",\n",
    "    \"Education\": r\"EDUCATION[\\s\\S]+\"\n",
    "}\n",
    "\n",
    "# Extract each section based on patterns\n",
    "structured_text = {}\n",
    "for section, pattern in sections.items():\n",
    "    match = re.search(pattern, extracted_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        structured_text[section] = match.group().strip()\n",
    "\n",
    "# Display structured text\n",
    "for section, content in structured_text.items():\n",
    "    print(f\"{section}:\\n{content}\\n{'-' * 50}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
